{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from Basilisk.utilities import (\n",
    "    SimulationBaseClass,\n",
    "    macros,\n",
    "    unitTestSupport,\n",
    "    simulationArchTypes\n",
    ")\n",
    "\n",
    "from Basilisk.simulation import (\n",
    "    spacecraft,\n",
    "    extForceTorque,\n",
    "    simpleNav\n",
    ")\n",
    "\n",
    "from Basilisk.fswAlgorithms import (\n",
    "    inertial3D,\n",
    "    attTrackingError\n",
    ")\n",
    "\n",
    "from Basilisk.architecture import messaging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, tumble, desired_orientation, sim_max_time, sim_dt, record=True, num_log_points = 150):\n",
    "        self.sim_task_name = \"simTask\"\n",
    "        self.sim_proc_name = \"simProc\"\n",
    "        \n",
    "        self.record = record\n",
    "        \n",
    "        self.sim = SimulationBaseClass.SimBaseClass()\n",
    "        self.sim_max_time = sim_max_time\n",
    "        self.sim_dt = sim_dt\n",
    "        \n",
    "        self.dyn_process = self.sim.CreateNewProcess(self.sim_proc_name, 10)\n",
    "        self.dyn_process.addTask(self.sim.CreateNewTask(self.sim_task_name, self.sim_dt))\n",
    "        \n",
    "        \n",
    "        # Setup the spaecraft model.\n",
    "        # The spacecraft model's documentation is found at\n",
    "        # http://hanspeterschaub.info/basilisk/Documentation/simulation/dynamics/spacecraft/spacecraft.html\n",
    "        self.spacecraft = spacecraft.Spacecraft()\n",
    "        self.spacecraft.ModelTag = \"bsk-Sat\"\n",
    "        \n",
    "        # Define the inertial properties\n",
    "        self.I = [900., 0., 0.,\n",
    "                  0., 800., 0.,\n",
    "                  0., 0., 600.]\n",
    "        \n",
    "        self.spacecraft.hub.mHub = 750.0  # spacecraft mass [kg]\n",
    "        self.spacecraft.hub.r_BcB_B = [[0.0], [0.0], [0.0]]  # m - position vector of body-fixed point B relative to CM\n",
    "        self.spacecraft.hub.IHubPntBc_B = unitTestSupport.np2EigenMatrix3d(self.I)\n",
    "        self.spacecraft.hub.sigma_BNInit = [[0.1], [0.2], [-0.3]]  # sigma_BN_B\n",
    "        self.spacecraft.hub.omega_BN_BInit = tumble  # [rad/s]\n",
    "        \n",
    "        # Add the spacecraft object to the simulation process\n",
    "        self.sim.AddModelToTask(self.sim_task_name, self.spacecraft)\n",
    "        \n",
    "        # Setup the external control torque\n",
    "        self.ex_torque = extForceTorque.ExtForceTorque()\n",
    "        self.ex_torque.ModelTag = \"externalDisturbance\"\n",
    "        self.spacecraft.addDynamicEffector(self.ex_torque)\n",
    "        self.sim.AddModelToTask(self.sim_task_name, self.ex_torque)\n",
    "        \n",
    "        # Setup the navigation sensor module which controls the\n",
    "        # craft's attitude, rate, and position\n",
    "        self.nav = simpleNav.SimpleNav()\n",
    "        self.nav.ModelTag = \"simpleNavigation\"\n",
    "        self.sim.AddModelToTask(self.sim_task_name, self.nav)\n",
    "        \n",
    "        # Setup the inertial 3D guidance module\n",
    "        self._i3D = inertial3D.inertial3DConfig()\n",
    "        self.i3D = self.sim.setModelDataWrap(self._i3D)\n",
    "        self.i3D.ModelTag = \"inertial3D\"\n",
    "        self.sim.AddModelToTask(self.sim_task_name, self.i3D, self._i3D)\n",
    "        self._i3D.sigma_R0N = desired_orientation\n",
    "        \n",
    "        # Setup the attitude tracking error evaluation module\n",
    "        self._attErr = attTrackingError.attTrackingErrorConfig()\n",
    "        self.attErr = self.sim.setModelDataWrap(self._attErr)\n",
    "        self.attErr.ModelTag = \"attErrorInertial3D\"\n",
    "        self.sim.AddModelToTask(self.sim_task_name, self.attErr, self._attErr)\n",
    "        \n",
    "        # Set up recording of values *before* the simulation is initialized\n",
    "        if self.record:\n",
    "            t = unitTestSupport.samplingTime(self.sim_max_time, self.sim_dt, num_log_points)\n",
    "            self.attitude_err_log = self._attErr.attGuidOutMsg.recorder(t)\n",
    "            self.sim.AddModelToTask(self.sim_task_name, self.attitude_err_log)\n",
    "        \n",
    "        # Set up the messaging\n",
    "        self.nav.scStateInMsg.subscribeTo(self.spacecraft.scStateOutMsg)\n",
    "        self._attErr.attNavInMsg.subscribeTo(self.nav.attOutMsg)\n",
    "        self._attErr.attRefInMsg.subscribeTo(self._i3D.attRefOutMsg)\n",
    "        \n",
    "    def set_external_torque_cmd_msg(self, msg):\n",
    "        self.ex_torque.cmdTorqueInMsg.subscribeTo(msg)\n",
    "        \n",
    "    def run(self):\n",
    "        self.sim.InitializeSimulation()\n",
    "        # self.sim.ConfigureStopTime(self.sim_max_time)\n",
    "        print(f\"Overall stop time is {self.sim_max_time}\")\n",
    "        \n",
    "        first_leg = self.sim_max_time / 2\n",
    "        print(f\"Simulating up until {first_leg}\")\n",
    "        self.sim.ConfigureStopTime(first_leg)\n",
    "        self.sim.ExecuteSimulation()\n",
    "        \n",
    "        print(f\"Simulating now until {self.sim_max_time}\")\n",
    "        self.sim.ConfigureStopTime(self.sim_max_time)\n",
    "        self.sim.ExecuteSimulation()\n",
    "        \n",
    "    def get_plot_data(self):\n",
    "        if not self.record:\n",
    "            print(\"WARNING: Sim did not record!\")\n",
    "            return\n",
    "        dataLr = self.mrp_log.torqueRequestBody\n",
    "        dataSigmaBR = self.attitude_err_log.sigma_BR\n",
    "        dataOmegaBR = self.attitude_err_log.omega_BR_B\n",
    "        timeAxis = self.attitude_err_log.times()\n",
    "        \n",
    "        return dataLr, dataSigmaBR, dataOmegaBR, timeAxis\n",
    "        \n",
    "    def plot(dataLr, dataSigmaBR, dataOmegaBR, timeAxis):\n",
    "        np.set_printoptions(precision=16)\n",
    "\n",
    "        plt.figure(1)\n",
    "        for idx in range(3):\n",
    "            plt.plot(timeAxis * macros.NANO2MIN, dataSigmaBR[:, idx],\n",
    "                     color=unitTestSupport.getLineColor(idx, 3),\n",
    "                     label=r'$\\sigma_' + str(idx) + '$')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlabel('Time [min]')\n",
    "        plt.ylabel(r'Attitude Error $\\sigma_{B/R}$')\n",
    "        figureList = {}\n",
    "        pltName = \"1\"\n",
    "        figureList[pltName] = plt.figure(1)\n",
    "\n",
    "        plt.figure(2)\n",
    "        for idx in range(3):\n",
    "            plt.plot(timeAxis * macros.NANO2MIN, dataLr[:, idx],\n",
    "                     color=unitTestSupport.getLineColor(idx, 3),\n",
    "                     label='$L_{r,' + str(idx) + '}$')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlabel('Time [min]')\n",
    "        plt.ylabel('Control Torque $L_r$ [Nm]')\n",
    "        pltName = \"2\" \n",
    "        figureList[pltName] = plt.figure(2)\n",
    "\n",
    "        plt.figure(3)\n",
    "        for idx in range(3):\n",
    "            plt.plot(timeAxis * macros.NANO2MIN, dataOmegaBR[:, idx],\n",
    "                     color=unitTestSupport.getLineColor(idx, 3),\n",
    "                     label=r'$\\omega_{BR,' + str(idx) + '}$')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlabel('Time [min]')\n",
    "        plt.ylabel('Rate Tracking Error [rad/s] ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gym environment to train RL-based control of attitude correction\n",
    "import gymnasium as gym\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "\n",
    "class AttitudeGym(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.action_space = gym.spaces.Box(low=-100, high=100, shape=(3,), dtype=float)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=float)\n",
    "        \n",
    "        self.simulation = None\n",
    "        self.step_size_ns = config['step_size_ns']\n",
    "        self.max_mission_time_ns = config['max_mission_time_ns']\n",
    "        self.run_until_ns = None\n",
    "        self.action_msg = None\n",
    "        self.record_sim = config['record_sim']\n",
    "        self.show_debug=config['show_debug']\n",
    "        self.iter = None\n",
    "        \n",
    "        self.tumble = [[0.8], [-0.6], [0.5]]\n",
    "        self.desired_ori = [0.0]*3\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.simulation = Simulation(self.tumble, self.desired_ori, self.max_mission_time_ns, self.step_size_ns, record=self.record_sim)\n",
    "        self.simulation.sim.InitializeSimulation()\n",
    "        \n",
    "        self.obs_space_recorder = self.simulation._attErr.attGuidOutMsg.recorder(self.step_size_ns)\n",
    "        self.simulation.sim.AddModelToTask(self.simulation.sim_task_name, self.obs_space_recorder)\n",
    "                \n",
    "        self.run_until_ns = self.step_size_ns\n",
    "        self.action_msg = messaging.CmdTorqueBodyMsg()\n",
    "        self.simulation.set_external_torque_cmd_msg(self.action_msg)\n",
    "        self.iter = 0\n",
    "        \n",
    "        self._run()\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        self._debug_msg(f\"Iteration {self.iter}\")\n",
    "        msgData = messaging.CmdTorqueBodyMsgPayload()\n",
    "        msgData.torqueRequestBody = action\n",
    "        self._debug_msg(f\"Publishing torque request = {action}\", tab=True)\n",
    "        self.action_msg.write(msgData)\n",
    "        self._run()\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        self._debug_msg(f\"Observation is {obs}\", tab=True)\n",
    "        done = self.run_until_ns >= self.max_mission_time_ns\n",
    "        reward = self._get_reward()\n",
    "        self._debug_msg(f\"Reward is {reward}\", tab=True)\n",
    "        truncated = done\n",
    "        \n",
    "        self.iter += 1\n",
    "        \n",
    "        return np.array(obs), reward, done, truncated, {}\n",
    "        \n",
    "    def render(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "        \n",
    "    def _run(self):\n",
    "        self.simulation.sim.ConfigureStopTime(self.run_until_ns)\n",
    "        self.simulation.sim.ExecuteSimulation()\n",
    "        self.run_until_ns += self.step_size_ns\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        sigma_BR_obs = self.obs_space_recorder.sigma_BR[-1]\n",
    "        omega_BR_B_obs = self.obs_space_recorder.omega_BR_B[-1]\n",
    "        \n",
    "        return list(it.chain.from_iterable([sigma_BR_obs, omega_BR_B_obs]))\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        # Reward the agent for getting the positional data closer to 0\n",
    "        sigma_BR = self.obs_space_recorder.sigma_BR[-1]\n",
    "        abs_delta = np.linalg.norm(self.desired_ori - sigma_BR)\n",
    "        if abs_delta <= 0.1:\n",
    "            self._debug_msg(\"Delta between current and desired orientaiton is small! Giving positve reward\")\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -10.0 * abs_delta\n",
    "        self._debug_msg(f\"sigma_BR = {sigma_BR}, desired orientation is {self.desired_ori}, delta is {self.desired_ori - sigma_BR}, and reward is {reward}\", tab=True)\n",
    "        return reward\n",
    "    \n",
    "    def _debug_msg(self, msg, tab=False):\n",
    "        if self.show_debug:\n",
    "            if tab:\n",
    "                msg = '\\t' + msg\n",
    "            print(f\"[DEBUG] {msg}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 16:52:12,791\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-03-14 16:52:13,896\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-03-14 16:52:13,896\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "/home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Iteration 0\n",
      "[DEBUG] \tPublishing torque request = [-14.60486248  58.24889049  35.07007697]\n",
      "[DEBUG] \tObservation is [0.12282912313149112, 0.14139647853446594, -0.2941996421491286, 0.7865614386308744, -0.6292689104384549, 0.4837449458071308]\n",
      "[DEBUG] \tsigma_BR = [ 0.12282912  0.14139648 -0.29419964], desired orientation is [0.0, 0.0, 0.0], delta is [-0.12282912 -0.14139648  0.29419964], and reward is -3.487597841951879\n",
      "[DEBUG] \tReward is -3.487597841951879\n",
      "AttitudeGym sanity check running...\n",
      "[DEBUG] Iteration 1\n",
      "[DEBUG] \tPublishing torque request = [-83.45585807  51.65126104  87.00298957]\n",
      "[DEBUG] \tObservation is [0.13310716266155515, 0.11214292982706298, -0.2907980228993171, 0.7781538871374984, -0.6361449507468878, 0.4813397099383056]\n",
      "[DEBUG] \tsigma_BR = [ 0.13310716  0.11214293 -0.29079802], desired orientation is [0.0, 0.0, 0.0], delta is [-0.13310716 -0.11214293  0.29079802], and reward is -3.3890565587514034\n",
      "[DEBUG] \tReward is -3.3890565587514034\n",
      "[DEBUG] Iteration 2\n",
      "[DEBUG] \tPublishing torque request = [ -6.26239293 -35.19364274  62.68095953]\n",
      "[DEBUG] \tObservation is [0.14265924522324816, 0.083001015898002, -0.2868271377319138, 0.7619914318503124, -0.6436790804043233, 0.4876271267710191]\n",
      "[DEBUG] \tsigma_BR = [ 0.14265925  0.08300102 -0.28682714], desired orientation is [0.0, 0.0, 0.0], delta is [-0.14265925 -0.08300102  0.28682714], and reward is -3.3092391244400803\n",
      "[DEBUG] \tReward is -3.3092391244400803\n",
      "[DEBUG] Iteration 3\n",
      "[DEBUG] \tPublishing torque request = [ 12.07738325 -21.95365763  53.53591214]\n",
      "[DEBUG] \tObservation is [0.15142894065497406, 0.053799903502579904, -0.2822900442472864, 0.754205215597668, -0.661972777620928, 0.4898254202999669]\n",
      "[DEBUG] \tsigma_BR = [ 0.15142894  0.0537999  -0.28229004], desired orientation is [0.0, 0.0, 0.0], delta is [-0.15142894 -0.0537999   0.28229004], and reward is -3.2482737379400377\n",
      "[DEBUG] \tReward is -3.2482737379400377\n",
      "[DEBUG] Iteration 4\n",
      "[DEBUG] \tPublishing torque request = [ 85.05695204  71.51590749 -40.62361284]\n",
      "[DEBUG] \tObservation is [0.1594740535580293, 0.02437110659780469, -0.277343468498052, 0.7482472584638657, -0.6785239327336041, 0.49035613297669545]\n",
      "[DEBUG] \tsigma_BR = [ 0.15947405  0.02437111 -0.27734347], desired orientation is [0.0, 0.0, 0.0], delta is [-0.15947405 -0.02437111  0.27734347], and reward is -3.2085093752950256\n",
      "[DEBUG] \tReward is -3.2085093752950256\n",
      "[DEBUG] Iteration 5\n",
      "[DEBUG] \tPublishing torque request = [  7.63756868 -97.63445357  14.97190804]\n",
      "[DEBUG] \tObservation is [0.16703304452147474, -0.005155674496131955, -0.27219664213487255, 0.7503943138642603, -0.6831484465344407, 0.4750825373252467]\n",
      "[DEBUG] \tsigma_BR = [ 0.16703304 -0.00515567 -0.27219664], desired orientation is [0.0, 0.0, 0.0], delta is [-0.16703304  0.00515567  0.27219664], and reward is -3.1940198955410857\n",
      "[DEBUG] \tReward is -3.1940198955410857\n",
      "[DEBUG] Iteration 6\n",
      "[DEBUG] \tPublishing torque request = [-16.78749016 -18.21570907 -22.57991969]\n",
      "[DEBUG] \tObservation is [0.17407322863018349, -0.034890046543086355, -0.2669850821040304, 0.7439438668554456, -0.7085785136135215, 0.46891215828717037]\n",
      "[DEBUG] \tsigma_BR = [ 0.17407323 -0.03489005 -0.26698508], desired orientation is [0.0, 0.0, 0.0], delta is [-0.17407323  0.03489005  0.26698508], and reward is -3.206241387350782\n",
      "[DEBUG] \tReward is -3.206241387350782\n",
      "[DEBUG] Iteration 7\n",
      "[DEBUG] \tPublishing torque request = [-64.95979162 -53.23606556 -66.9870273 ]\n",
      "[DEBUG] \tObservation is [0.18041374623335202, -0.06492234523206508, -0.2617149231650863, 0.734716296470987, -0.7236819842677156, 0.4563242767231434]\n",
      "[DEBUG] \tsigma_BR = [ 0.18041375 -0.06492235 -0.26171492], desired orientation is [0.0, 0.0, 0.0], delta is [-0.18041375  0.06492235  0.26171492], and reward is -3.244360210391115\n",
      "[DEBUG] \tReward is -3.244360210391115\n",
      "[DEBUG] Iteration 8\n",
      "[DEBUG] \tPublishing torque request = [-49.08805656  95.52292736  58.69406556]\n",
      "[DEBUG] \tObservation is [0.18607024994868185, -0.0951106964229422, -0.25655434180502235, 0.720227862022432, -0.7425124123981928, 0.4362710334882]\n",
      "[DEBUG] \tsigma_BR = [ 0.18607025 -0.0951107  -0.25655434], desired orientation is [0.0, 0.0, 0.0], delta is [-0.18607025  0.0951107   0.25655434], and reward is -3.308901823702696\n",
      "[DEBUG] \tReward is -3.308901823702696\n",
      "[DEBUG] Iteration 9\n",
      "[DEBUG] \tPublishing torque request = [ -6.96205319 -85.05102566 -15.00447101]\n",
      "[DEBUG] \tObservation is [0.19107575236685953, -0.12535561227722325, -0.251270639072635, 0.70756849070188, -0.7422638647973407, 0.4372200203660499]\n",
      "[DEBUG] \tsigma_BR = [ 0.19107575 -0.12535561 -0.25127064], desired orientation is [0.0, 0.0, 0.0], delta is [-0.19107575  0.12535561  0.25127064], and reward is -3.396482102586872\n",
      "[DEBUG] \tReward is -3.396482102586872\n",
      "[DEBUG] Iteration 10\n",
      "[DEBUG] \tPublishing torque request = [ -8.55418631  -4.5880544  -47.38244762]\n",
      "[DEBUG] \tObservation is [0.1954837935640066, -0.15581231359790956, -0.24577694760538382, 0.6995708906417656, -0.7642819387651237, 0.4258860789520252]\n",
      "[DEBUG] \tsigma_BR = [ 0.19548379 -0.15581231 -0.24577695], desired orientation is [0.0, 0.0, 0.0], delta is [-0.19548379  0.15581231  0.24577695], and reward is -3.505676804685909\n",
      "[DEBUG] \tReward is -3.505676804685909\n",
      "[DEBUG] Iteration 11\n",
      "[DEBUG] \tPublishing torque request = [ 55.03781535 -32.902657    56.38416948]\n",
      "[DEBUG] \tObservation is [0.19938215575363893, -0.18662400514260877, -0.2403583931806219, 0.691476731215756, -0.7757443244840306, 0.40906273997578885]\n",
      "[DEBUG] \tsigma_BR = [ 0.19938216 -0.18662401 -0.24035839], desired orientation is [0.0, 0.0, 0.0], delta is [-0.19938216  0.18662401  0.24035839], and reward is -3.638047835045703\n",
      "[DEBUG] \tReward is -3.638047835045703\n",
      "[DEBUG] Iteration 12\n",
      "[DEBUG] \tPublishing torque request = [-58.82127603   3.31397772 -46.14596326]\n",
      "[DEBUG] \tObservation is [0.20284718431004015, -0.21785458412354616, -0.23480545013470105, 0.6904698769179316, -0.7904620094662861, 0.40944157426902855]\n",
      "[DEBUG] \tsigma_BR = [ 0.20284718 -0.21785458 -0.23480545], desired orientation is [0.0, 0.0, 0.0], delta is [-0.20284718  0.21785458  0.23480545], and reward is -3.79132166162559\n",
      "[DEBUG] \tReward is -3.79132166162559\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Test the basic functionality of the Gym abstraction\n",
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "env_config = {\n",
    "    'step_size_ns': macros.sec2nano(0.1), \n",
    "    'max_mission_time_ns': macros.sec2nano(10.0), \n",
    "    'record_sim': False, \n",
    "    'show_debug': True\n",
    "}\n",
    "\n",
    "g = AttitudeGym(env_config)\n",
    "ray.rllib.utils.check_env(g)\n",
    "print(\"AttitudeGym sanity check running...\")\n",
    "for i in it.count():\n",
    "    obs, reward, done, trunc, info = g.step(g.action_space.sample())\n",
    "    if done or i > 10:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "        \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 16:52:20,536\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Torch model that just delegates a fully connected net\n",
    "import ray\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_torch, try_import_tf\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "    \n",
    "ModelCatalog.register_custom_model(\n",
    "    \"my_model\", TorchCustomModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '_disable_action_flattening': False,\n",
      "    '_disable_execution_plan_api': True,\n",
      "    '_disable_preprocessor_api': False,\n",
      "    '_enable_rl_module_api': False,\n",
      "    '_enable_rl_trainer_api': False,\n",
      "    '_fake_gpus': False,\n",
      "    '_rl_trainer_hps': RLTrainerHPs(),\n",
      "    '_tf_policy_handles_more_than_one_loss': False,\n",
      "    'action_space': None,\n",
      "    'actions_in_input_normalized': False,\n",
      "    'always_attach_evaluation_results': False,\n",
      "    'auto_wrap_old_gym_envs': True,\n",
      "    'batch_mode': 'complete_episodes',\n",
      "    'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
      "    'checkpoint_trainable_policies_only': False,\n",
      "    'clip_actions': False,\n",
      "    'clip_param': 0.3,\n",
      "    'clip_rewards': None,\n",
      "    'compress_observations': False,\n",
      "    'create_env_on_driver': False,\n",
      "    'custom_eval_function': None,\n",
      "    'custom_resources_per_worker': {},\n",
      "    'disable_env_checking': False,\n",
      "    'eager_max_retraces': 20,\n",
      "    'eager_tracing': False,\n",
      "    'enable_async_evaluation': False,\n",
      "    'enable_connectors': True,\n",
      "    'enable_tf1_exec_eagerly': False,\n",
      "    'entropy_coeff': 0.0,\n",
      "    'entropy_coeff_schedule': None,\n",
      "    'env': <class '__main__.AttitudeGym'>,\n",
      "    'env_config': {   'max_mission_time_ns': 10000000000,\n",
      "                      'record_sim': False,\n",
      "                      'show_debug': False,\n",
      "                      'step_size_ns': 100000000},\n",
      "    'env_task_fn': None,\n",
      "    'evaluation_config': None,\n",
      "    'evaluation_duration': 10,\n",
      "    'evaluation_duration_unit': 'episodes',\n",
      "    'evaluation_interval': None,\n",
      "    'evaluation_num_workers': 0,\n",
      "    'evaluation_parallel_to_training': False,\n",
      "    'evaluation_sample_timeout_s': 180.0,\n",
      "    'exploration_config': {'type': 'StochasticSampling'},\n",
      "    'explore': True,\n",
      "    'export_native_model_files': False,\n",
      "    'extra_python_environs_for_driver': {},\n",
      "    'extra_python_environs_for_worker': {},\n",
      "    'fake_sampler': False,\n",
      "    'framework': 'torch',\n",
      "    'gamma': 0.99,\n",
      "    'grad_clip': None,\n",
      "    'horizon': -1,\n",
      "    'ignore_worker_failures': False,\n",
      "    'in_evaluation': False,\n",
      "    'input': 'sampler',\n",
      "    'input_config': {},\n",
      "    'is_atari': None,\n",
      "    'keep_per_episode_custom_metrics': False,\n",
      "    'kl_coeff': 0.2,\n",
      "    'kl_target': 0.01,\n",
      "    'lambda': 1.0,\n",
      "    'local_tf_session_args': {   'inter_op_parallelism_threads': 8,\n",
      "                                 'intra_op_parallelism_threads': 8},\n",
      "    'log_level': 'WARN',\n",
      "    'log_sys_usage': True,\n",
      "    'logger_config': None,\n",
      "    'logger_creator': None,\n",
      "    'lr': 5e-05,\n",
      "    'lr_schedule': None,\n",
      "    'max_requests_in_flight_per_sampler_worker': 2,\n",
      "    'metrics_episode_collection_timeout_s': 60.0,\n",
      "    'metrics_num_episodes_for_smoothing': 100,\n",
      "    'min_sample_timesteps_per_iteration': 0,\n",
      "    'min_time_s_per_iteration': None,\n",
      "    'min_train_timesteps_per_iteration': 0,\n",
      "    'model': {   '_disable_action_flattening': False,\n",
      "                 '_disable_preprocessor_api': False,\n",
      "                 '_time_major': False,\n",
      "                 '_use_default_native_models': -1,\n",
      "                 'attention_dim': 64,\n",
      "                 'attention_head_dim': 32,\n",
      "                 'attention_init_gru_gate_bias': 2.0,\n",
      "                 'attention_memory_inference': 50,\n",
      "                 'attention_memory_training': 50,\n",
      "                 'attention_num_heads': 1,\n",
      "                 'attention_num_transformer_units': 1,\n",
      "                 'attention_position_wise_mlp_dim': 32,\n",
      "                 'attention_use_n_prev_actions': 0,\n",
      "                 'attention_use_n_prev_rewards': 0,\n",
      "                 'conv_activation': 'relu',\n",
      "                 'conv_filters': None,\n",
      "                 'custom_action_dist': None,\n",
      "                 'custom_model': 'my_model',\n",
      "                 'custom_model_config': {},\n",
      "                 'custom_preprocessor': None,\n",
      "                 'dim': 84,\n",
      "                 'fcnet_activation': 'tanh',\n",
      "                 'fcnet_hiddens': [256, 256],\n",
      "                 'framestack': True,\n",
      "                 'free_log_std': False,\n",
      "                 'grayscale': False,\n",
      "                 'lstm_cell_size': 256,\n",
      "                 'lstm_use_prev_action': False,\n",
      "                 'lstm_use_prev_action_reward': -1,\n",
      "                 'lstm_use_prev_reward': False,\n",
      "                 'max_seq_len': 20,\n",
      "                 'no_final_linear': False,\n",
      "                 'post_fcnet_activation': 'relu',\n",
      "                 'post_fcnet_hiddens': [],\n",
      "                 'use_attention': False,\n",
      "                 'use_lstm': False,\n",
      "                 'vf_share_layers': True,\n",
      "                 'zero_mean': True},\n",
      "    'multiagent': {   'count_steps_by': 'env_steps',\n",
      "                      'observation_fn': None,\n",
      "                      'policies': {'default_policy': (None, None, None, None)},\n",
      "                      'policies_to_train': None,\n",
      "                      'policy_map_cache': -1,\n",
      "                      'policy_map_capacity': 100,\n",
      "                      'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x7f34ec3e6d40>},\n",
      "    'no_done_at_end': -1,\n",
      "    'normalize_actions': True,\n",
      "    'num_consecutive_worker_failures_tolerance': 100,\n",
      "    'num_cpus_for_driver': 1,\n",
      "    'num_cpus_per_trainer_worker': 1,\n",
      "    'num_cpus_per_worker': 1,\n",
      "    'num_envs_per_worker': 1,\n",
      "    'num_gpus': 1,\n",
      "    'num_gpus_per_trainer_worker': 0,\n",
      "    'num_gpus_per_worker': 0,\n",
      "    'num_sgd_iter': 30,\n",
      "    'num_trainer_workers': 0,\n",
      "    'num_workers': 15,\n",
      "    'observation_filter': 'NoFilter',\n",
      "    'observation_space': None,\n",
      "    'off_policy_estimation_methods': {},\n",
      "    'offline_sampling': False,\n",
      "    'ope_split_batch_by_episode': True,\n",
      "    'optimizer': {},\n",
      "    'output': None,\n",
      "    'output_compress_columns': ['obs', 'new_obs'],\n",
      "    'output_config': {},\n",
      "    'output_max_file_size': 67108864,\n",
      "    'placement_strategy': 'PACK',\n",
      "    'policies': {   'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7f3599261840>},\n",
      "    'policy_states_are_swappable': False,\n",
      "    'postprocess_inputs': False,\n",
      "    'preprocessor_pref': 'deepmind',\n",
      "    'recreate_failed_workers': False,\n",
      "    'remote_env_batch_wait_ms': 0,\n",
      "    'remote_worker_envs': False,\n",
      "    'render_env': False,\n",
      "    'replay_sequence_length': None,\n",
      "    'restart_failed_sub_environments': False,\n",
      "    'rl_module_class': None,\n",
      "    'rl_trainer_class': None,\n",
      "    'rollout_fragment_length': 'auto',\n",
      "    'sample_async': False,\n",
      "    'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "    'sampler_perf_stats_ema_coef': None,\n",
      "    'seed': None,\n",
      "    'sgd_minibatch_size': 128,\n",
      "    'shuffle_buffer_size': 0,\n",
      "    'shuffle_sequences': True,\n",
      "    'simple_optimizer': -1,\n",
      "    'soft_horizon': -1,\n",
      "    'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
      "    'synchronize_filters': True,\n",
      "    'tf_session_args': {   'allow_soft_placement': True,\n",
      "                           'device_count': {'CPU': 1},\n",
      "                           'gpu_options': {'allow_growth': True},\n",
      "                           'inter_op_parallelism_threads': 2,\n",
      "                           'intra_op_parallelism_threads': 2,\n",
      "                           'log_device_placement': False},\n",
      "    'train_batch_size': 4000,\n",
      "    'use_critic': True,\n",
      "    'use_gae': True,\n",
      "    'validate_workers_after_construction': True,\n",
      "    'vf_clip_param': 10.0,\n",
      "    'vf_loss_coeff': 1.0,\n",
      "    'vf_share_layers': -1,\n",
      "    'worker_cls': None,\n",
      "    'worker_health_probe_timeout_s': 60,\n",
      "    'worker_restore_timeout_s': 1800}\n"
     ]
    }
   ],
   "source": [
    "# Define trainer config for Ray training algorithm\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    # or \"corridor\" if registered above\n",
    "    .environment(AttitudeGym, env_config={\n",
    "        'step_size_ns': macros.sec2nano(0.1), \n",
    "        'max_mission_time_ns': macros.sec2nano(10.0), \n",
    "        'record_sim': False, \n",
    "        'show_debug': False\n",
    "    })\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=os.cpu_count()-1, batch_mode=\"complete_episodes\")\n",
    "    .training(\n",
    "        model={\n",
    "            \"custom_model\": \"my_model\",\n",
    "            \"vf_share_layers\": True,\n",
    "        },\n",
    "        #train_batch_size=10000\n",
    "    )\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    #.resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    .resources(num_gpus=1)\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(config.to_dict())\n",
    "\n",
    "# Define stop conditions\n",
    "stop_cond = {\n",
    "    \"episode_reward_mean\" : 100,  # this is somewhat random\n",
    "    \"training_iteration\": 1500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-14 16:40:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:08:55.54        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.6/15.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/7.26 GiB heap, 0.0/3.63 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_AttitudeGym_69621_00000</td><td>RUNNING </td><td>192.168.1.52:16421</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         519.957</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">-502.825</td><td style=\"text-align: right;\">            -475.071</td><td style=\"text-align: right;\">            -528.549</td><td style=\"text-align: right;\">                98</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=16421)\u001b[0m 2023-03-14 16:32:00,677\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=16421)\u001b[0m 2023-03-14 16:32:00,857\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16505)\u001b[0m 2023-03-14 16:32:07,155\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16505)\u001b[0m 2023-03-14 16:32:07,155\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16505)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16505)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16511)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16511)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16510)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16510)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16603)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16603)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16506)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16506)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16507)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16507)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16587)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16587)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16509)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16509)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16658)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16658)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16636)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16636)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16604)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16604)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16749)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16749)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16656)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16656)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16657)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16657)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16855)\u001b[0m /home/spacetrexlab/.pyenv/versions/basilisk/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16855)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                              </th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                                   </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                 </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_AttitudeGym_69621_00000</td><td style=\"text-align: right;\">                 572000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007863759994506836, &#x27;StateBufferConnector_ms&#x27;: 0.0048639774322509766, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1331179141998291}</td><td>{&#x27;num_env_steps_sampled&#x27;: 572000, &#x27;num_env_steps_trained&#x27;: 572000, &#x27;num_agent_steps_sampled&#x27;: 572000, &#x27;num_agent_steps_trained&#x27;: 572000}</td><td>{}              </td><td>2023-03-14_16-40-53</td><td>False </td><td style=\"text-align: right;\">                98</td><td>{}             </td><td style=\"text-align: right;\">            -475.071</td><td style=\"text-align: right;\">             -502.825</td><td style=\"text-align: right;\">            -528.549</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">            5830</td><td>f884378c0f21413b9a27bb8d4e76fa3c</td><td>rostration</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;custom_metrics&#x27;: {}, &#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 1.7085937499999995, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 9.251879592095651, &#x27;policy_loss&#x27;: 0.012570000792382865, &#x27;vf_loss&#x27;: 9.225919731201664, &#x27;vf_explained_var&#x27;: 0.01958377181842763, &#x27;kl&#x27;: 0.007836763369876008, &#x27;entropy&#x27;: 5.2169268069728725, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;num_grad_updates_lifetime&#x27;: 132525.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 572000, &#x27;num_env_steps_trained&#x27;: 572000, &#x27;num_agent_steps_sampled&#x27;: 572000, &#x27;num_agent_steps_trained&#x27;: 572000}</td><td style=\"text-align: right;\">                       143</td><td>192.168.1.52</td><td style=\"text-align: right;\">                   572000</td><td style=\"text-align: right;\">                   572000</td><td style=\"text-align: right;\">                 572000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                 572000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   15</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 28.459999999999997, &#x27;ram_util_percent&#x27;: 75.0, &#x27;gpu_util_percent0&#x27;: 0.20400000000000001, &#x27;vram_util_percent0&#x27;: 0.11789025306940618}</td><td style=\"text-align: right;\">16421</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.3716388306783318, &#x27;mean_inference_ms&#x27;: 0.9743967941937052, &#x27;mean_action_processing_ms&#x27;: 0.19015606776123728, &#x27;mean_env_wait_ms&#x27;: 1.8058971937511696, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -475.07107021582135, &#x27;episode_reward_min&#x27;: -528.5492615239958, &#x27;episode_reward_mean&#x27;: -502.82533281218355, &#x27;episode_len_mean&#x27;: 98.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 45, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-509.46458325784613, -490.00472826675474, -514.207349880867, -484.6124753854084, -512.6711838604322, -500.7591366407024, -511.53008926771133, -523.9118264066032, -496.62787992121406, -521.822154400335, -493.82323796387897, -500.4716139044604, -528.5492615239958, -503.1504142265479, -491.1796628244419, -518.4953066765686, -512.2920778886798, -515.6076020159624, -500.95247766913815, -483.5867239930379, -494.980779015058, -506.64449763764344, -523.0490969684353, -499.52419387244765, -506.7924410447222, -484.2488094478404, -479.3088607606747, -520.0138142240393, -496.42503186258193, -514.2678079391093, -519.5783880313294, -499.0735738915135, -501.27197589531323, -491.87909971717863, -489.6673483792371, -512.8499313125494, -520.4011669127683, -502.0165570897154, -524.00857388096, -509.18061593363376, -508.26550256458137, -488.063975324829, -518.5687123132774, -494.40896270341426, -519.2208278116942, -500.72250276942407, -485.67334916379997, -502.89962448807717, -502.2253076424879, -517.7889551588468, -511.4937134457504, -502.7533544193354, -496.3158479628517, -505.65662502333856, -519.7132827537805, -486.5642258966418, -486.92658761926776, -503.62833854643696, -502.26817696322775, -496.3585046082698, -498.62275350533747, -495.5110274772045, -496.1005355932824, -516.4535256815458, -498.6204434595634, -499.3298449959769, -511.78647204662906, -518.3230297320422, -515.9760084114018, -515.0107343758608, -483.95029569295656, -496.354822593978, -490.6938732818286, -492.2208984147803, -517.4550981987009, -500.18362331715235, -497.61791870532807, -488.44797700647325, -481.03550439243224, -517.4087237487914, -504.6746257579699, -504.32475715401006, -499.92805933568263, -475.07107021582135, -511.4754328017379, -486.29515530543307, -492.05664934328706, -489.82737038539085, -486.62547372895574, -516.8116764655084, -496.1978710689083, -526.527600730271, -495.6164563385924, -501.9097456422129, -495.8846555932226, -503.0134918497444, -498.90411484660825, -500.895101726957, -510.5767279869895, -496.39540733911474], &#x27;episode_lengths&#x27;: [98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.3716388306783318, &#x27;mean_inference_ms&#x27;: 0.9743967941937052, &#x27;mean_action_processing_ms&#x27;: 0.19015606776123728, &#x27;mean_env_wait_ms&#x27;: 1.8058971937511696, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.007863759994506836, &#x27;StateBufferConnector_ms&#x27;: 0.0048639774322509766, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1331179141998291}}</td><td style=\"text-align: right;\">             519.957</td><td style=\"text-align: right;\">            3.6807</td><td style=\"text-align: right;\">       519.957</td><td>{&#x27;training_iteration_time_ms&#x27;: 3735.267, &#x27;load_time_ms&#x27;: 1.03, &#x27;load_throughput&#x27;: 3882536.333, &#x27;learn_time_ms&#x27;: 2750.955, &#x27;learn_throughput&#x27;: 1454.041, &#x27;synch_weights_time_ms&#x27;: 7.779}</td><td style=\"text-align: right;\"> 1678837253</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">           572000</td><td style=\"text-align: right;\">                 143</td><td style=\"text-align: right;\">69621_00000</td><td style=\"text-align: right;\">      8.59493</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform the training!\n",
    "from ray import air, tune\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop=stop_cond)\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "check_learning_achieved(results, stop_cond['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basilisk-env",
   "language": "python",
   "name": "basilisk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
